# 课堂纪要

## 第一次课 9月16日

各位亲爱的同学们大家好，首先非常欢迎大家选修汉生老师的《深度学习与人工智能》课程，这个课程的火爆程度超出了我们的想象。以下是今天的课程内容的简介，主要包括两个部分：
1. 第一部分：深度学习简介
	深度学习作为一种非线性回归，与经典的线性回归的区别在于我们选用了特殊的X（例如图像），并且建立了从X到Y的回归关系，例如人脸识别中的X是人脸，Y是人的身份，从X到Y建立回归关系后就可以判断这张人脸照片是否是某位特定的人
2. 第二部分：图像处理
	图像处理，包括将图片读入计算机、图片展示、对图片进行代数运算等

## 第二次课 9月23日

各位亲爱的同学们大家好！感谢大家今天的努力学习。接下来对今天的知识要点做一个总结。我们主要讲了如何通过Tensorflow实现线性回归。
1. 第一部分：线性回归回顾
   （1）线性回归模型形如：Y = Xb + e，用来处理连续响应变量的问题。（2）建立深度学习模型的方法论流程为：(a) 定义X和Y；(b) 定义X和Y之间的函数形式f，在线性回归中是线性的、将来会很复杂；(c) 定义损失函数来评价模型好坏；(d) 优化求解参数，在深度学习中不同的优化算法结果也会不同。 
2. 第二部分：数据准备工作
   （1）准备X数据： 首先初始化一个tensor X，采用Image.open()，从文件夹路径中循环读入图片，将其转换成numpy array，然后放入X中。由于我们的数据比较小，所以我们目前可以采用这样的方式读取数据，当数据量过大时，我们需要一块一块的读取数据。（2）准备Y数据：将csv文件中的因变量转换成矩阵(N*1)后赋值给Y，并对其进行中心化、标准化。（3）切分训练集和验证集：从sklearn.model_selection导入train_test_split函数，采用train_test_split()，将数据一分为二，一部分训练模型，一部分验证模型。
3. 第三部分：Tensorflow实现
   （1）建立模型：从Keras库中导入所需要的层和Model，进行相应操作。（2）模型结构：模型包括Input层、Flatten层和Dense层，Input层输入tensor的维度和对应size（第一个维度和样本量或batch_size对应），Flatten层将维数复杂的tensor拉直成一维向量，Dense层将输入的全体神经元以线性组合的方式全连接到一个节点输出。最后将input_layer和output_layer交给Model声明，我们就定义了一个f(X)。Tensorflow模型的流程特别像组装水管系统，数据是在水管（模型）中流动的水，不同的层是水管和水管之间的阀门，它们对流经此处的数据流进行相应的处理，最终在水管的末端得到输出，也就是f(X)。（3）施工方案：通过命令model.compile()，我们可以指定优化所需要的损失函数（MSE），优化器（Adam或SGD）和监测方案（检测指标MSE）。

## 第三次课 9月30日

大家好！感谢大家今天的努力学习。接下来对今天的知识要点做一个总结。我们主要讲了如何通过Tensorflow实现逻辑回归。
1. 一、逻辑回归回顾：
   不同于线性回归，逻辑回归中我们的Y是一个分类型的因变量。（1） 逻辑回归模型形如：
    $$ P(Y=k)=\exp(X^\top \beta_k)/ \sum_{k'} \exp(X^\top \beta_{k'}), $$ 
   在深度学习任务中X通常不是向量而是一个Tensor，我们需要参数的形状需要和X进行对齐(也需要是一个Tensor)。（2）建立深度学习模型的方法论流程为：(a) 定义X和Y，(b) 定义X和Y之间的函数形式f，(c) 定义损失函数来评价模型好坏，(d) 优化求解参数。 

2. 二、数据准备工作：
   （1）准备X数据： 首先初始化一个tensor X，采用Image.open()，从文件夹路径中循环读入图片，将其转换成矩阵，然后放入X中。由于我们的数据比较小，所以我们目前可以采用这样的方式读取数据，当数据量过大时，我们需要一块一块的读取数据。同学们要尝试理解硬盘存储、内存和显存的关系。（2）准备Y数据：将csv中的因变量转换成矩阵(N*1)后赋值给Y（应用to_categorical()将其转化为One-Hot型因变量）。（4）切分训练集和验证集：从sklearn.model_selection 导入 train_test_split，采用train_test_split()，将数据一分为二，一部分训练模型，一部分验证模型。
3. 三、Tensorflow的实现：
   （1）建立模型：从Keras库中导入所需要的层和Model，进行相应操作。（2）模型结构：模型包括Input层、Flatten层和Dense层，Input层输入tensor的维度和对应size（第一个维度和样本量or batch_size对应），Flatten层将维数复杂的tensor拉直成一维向量，Dense层将输入的全体神经元以线性组合的方式全连接到一个节点输出。最后将input_layer 和 output_layer叫给Model声明，我们就定义了一个f(X)。Tensorflow模型的流程特别像组装水管系统，数据是在水管（模型）中流动的水，不同的层是水管和水管之间的阀门，它们对流经此处的数据流进行相应的处理，最终在水管的末端得到输出，也就是f(X)。（4）施工方案：通过命令model.compile()，我们可以指定优化所需要的损失函数（因变量为One-Hot型使用"categorical_crossentropy"，否则使用"sparse_categorical_crossentropy"），优化器（Adam或SGD）和监测方案（检测指标"accuracy"）。

## 第四次课 10月7日

恭喜大家又勇敢地向前走出了一大步，相信后面的学习会越来越顺畅。今天主要讲了三个部分：梯度下降、卷积以及池化

1. 第一部分：梯度下降
我们从之前学过的简单模型：线性回归和逻辑回归出发，统一到优化损失函数的理论框架。我们从理论上建立了学习率大小和数值收敛的约束关系。

2. 第二部分：卷积 
   - 什么是卷积？为什么需要进行卷积操作？卷积就是计算某种局部特征的相似性。简单来说，有一个记录了目标图像局部特征的像素矩阵，称作【卷积核（convolution kernel）】或者【滤波器（filter）】。 我们通过其与搜索区域进行对应位置的点乘再求和运算，来量化两者之间的相似度。如果将卷积核先行后列地扫描某一图像的像素矩阵并进行运算，由此得到一个新的像素矩阵，整个过程就叫做卷积。 注意：因为卷积核一般关注的是局部特征，所以尺寸一般比较小，如3*3，5*5等。图像具有某种平移不变形，例如不同位置的熊头都是熊头，使用卷积可以衡量这种平移不变形。此外，如果所有模块全都使用全连接层，消耗的参数过多，而卷积相对而言较为“节约”。
   - 常见的卷积操作有full, same和valid卷积。这三种卷积其实是对卷积核移动范围的不同限制。 （a）full卷积：只要像素矩阵与卷积核元素有一个位置重叠，就要将对应位置的值相乘再求和（落在像素矩阵外的元素视为0） （b）same卷积： 能使输入和输出的像素矩阵尺寸一样（但不一定总是一样，和卷积核的步长也有关） （c）valid 卷积：要求卷积核全部被像素矩阵覆盖时，才能进行卷积运算 
   - 多通道卷积：如果输入的像素矩阵是多通道的，卷积核的通道个数要与输入保持一致。具体做法是分别在每一层通道上进行二维卷积，然后再进行深度方向的求和，最终输出一个单通道的矩阵，因此，该操作具有降维的作用。
   - 一个张量与多个卷积核的卷积：通常会使用多个卷积核进行卷积提取图像特征，再把每个卷积核的输出进行叠加，形成新的输出。最后输出的通道的个数与卷积核的个数一致。 一般操作之后，一个像素高通道数小的张量，变成像素低通道数大的张量，这样就提取到了更多的深层特征。 
   - 卷积的权重如何确定：卷积的权重是未知的参数，对于该参数的估计，需要先构造一个损失函数，根据某算法（比如SGD）优化后（即最小化损失函数），就得到卷积权重的估计。 

3. 第三部分：池化： 
   - 卷积和池化往往会同时出现，池化操作是对卷积得到的结果进一步处理，它是将平面内某一位置及其相邻位置的特征值进行统计汇总，并将汇总后的结果作为这一位置在该平面内的值输出。如果取最大值，则称为最大值池化，如果取平均值，则称为平均值池化。最常用的为最大池化，即在卷积中计算出图像局部特征的相似度，再通过池化挑出最突出的特征。 
   - 池化可以将维数大大降低，输出的维数=输入的维数/池子的大小。另外，池化不消耗参数。 
   - 与卷积一样，池化也分为same池化和valid池化等。
   - 多通道的池化：与卷积不同，通常池化会在每一通道上分别进行池化，最后输出的通道数和输入的通道数一致。

感谢大家的辛苦付出，继续加油！

## 第五次课 10月14日

亲爱的各位同学，大家好！我们今天的课程进入到了卷积神经网络（CNN）的世界，介绍了一个经典的CNN模型：LeNet5，以及一种重要的训练技巧：数据增强。

1. 第一部分：LeNet5模型
   - LeNet5是1998年由Yann LeCun等人提出的用于手写数字识别的CNN网络，是最经典的CNN网络之一。LeNet5的网络结构详解，包括各层的输出大小和消耗的参数个数时，同学们需要认真理解。易混淆点：卷积层输出的通道个数与卷积核的个数一致，池化层输出的通道个数与输入的通道个数一致。
   - Dense层为全连接层，这种结构产生了大量的参数，因此考虑使用Dropout将模型简化，即随机丢弃用于提取特征的神经元，进而减少模型需要估计的参数。
   - 目前使用的LeNet5 结构与最初提出的结构有所不同，例如选用的激活函数。现在一般使用 ReLU 作为激活函数，输出层则为 softmax。ReLU（Rectified Linear Units）为分段线性函数，函数形式为R(x)=max(x,0)。ReLU的特点在于，它能使某些神经元的输出为0，继而实现网络的稀疏性和输出的稳定。从统计学的角度来看，ReLU是一类样条函数，其线性组合可以逼近任意线性或非线性函数，从而增加神经网络的非线性表达能力。
2. 第二部分：数据增强（Data Augmentation）
   - 数据增强的目的在于，我们拿到的图像数据往往只是一个角度。为了让模型能够识别不同角度的相同图像，我们需要喂给模型不同角度的图像作为输入，这通过图像变换来实现。
   - 这些图像变换包括：放大/缩小、平移、旋转、拉伸等等。
   - 在代码实现上，我们采用Keres的数据生成器ImageData Generator批量地生成数据增强后的图像。数据生成器是一个类似搬运工的角色，他可以完成：按批量读取数据、数据归一化、统一图像尺寸、数据增强等一系列准备工作，便于我们后续训练。
   - 我们在处理训练数据时，需要进行数据增强；但在处理验证数据集时，不需要数据增强。

最后提醒一下，本周作业TASK5的截止时间是【10月21日】。
感谢大家的辛苦付出，继续加油！

## 第六次课 10月21日

亲爱的各位同学，经过了两个周的学习，我们终于开始接触到深度学习的一些独特的地方了。以下是今天的课堂纪要。今天我们主要学习了深度学习的重要技巧：Batch Normalization以及迁移学习。

第一部分：Batch Normalization 1.Batch Normalization 是 2015 年 Google 研究员提出来的，同时他也将这个方法用在了GoogleNet上。 2.我们用一个经典的猫狗大战数据来进行学习。 (1) 同学们要深刻理解“硬盘->内存->显存”。数据是存在硬盘上的，在计算时，图片先被读入内存，再读入显存，进行GPU运算。没有显存时（典型报错：某些tensor不能COPY，Resource exhausted OOM等），一因为模型参数过多，解决方案是用更简单的模型；二因为数据太大（batch_size*图片分辨率），解决方案是减少batch_size；三是计算冗余，这块消耗很不清楚。 (2) 数据生成：读入数据需要按照固定结构设置好目录形式。在路径下分为训练集和验证集，两个集下分别有cats和dogs两类，特别注意每个集下的类别文件名要保持一致。用ImageDataGenerator分批读入数据。 在数据生成器读取数据过程中，可以进行resize，batchsize等设置和调整。用next输出进行检查。 3.Batch Normalization 在传统的数据分析中，已被广为验证的一个方法就是把数据适当的标准化，这样模型会变得更加稳定。最简单的标准化方法是将数据变为均值为 0，标准差为 1。如果把一个 batch每层不同通道的像素取值看作一个整体，Batch Normalization 的核心就是让像素取值变为均值为 0，方差为 1。 Batch Normalization的做法如下： (1)算出样本均值 (2)算出样本方差 (3)标准化：减均值除以方差。为了防止方差为0，会在方差上加上一个很小很小的正数。 (4)做一个基本的线性变化（参数为γ和β），防止输出被激活函数全部变为0。 这里有四个参数：其中均值和方差不需要训练，γ和β需要优化，即通道需要四个参数，Non-trainable params为2。 4.我们考虑了三个加上了BN的模型，即普通的逻辑回归、宽度模型、深度模型，希望大家在这些基础上可以考虑如何改造。4.介绍了研究前沿论文:深度神经网络的因子归一化方法，有兴趣的同学可以学习[论文](https://www.nature.com/articles/s41598-022-09910-6)或观看学术报告（点击下方狗熊会推送）。

第二部分：迁移学习 1.为什么使用迁移学习：相信大家在过去的学习中已经体会到了训练模型（炼丹）的不易，一个良好的结果往往离不开充足的训练集以及成本不菲的训练。但同时也了解到已经有牛人搭建并训练出很多效果强大的经典模型，迁移学习能让我们站在牛人的肩膀上，将经典模型的知识与信息应用于不同的但相关的训练任务。2.迁移学习原理：在应用迁移学习时，保持源模型的一部分或全部层参数不变（冻结frozen），然后，在新任务的数据集上进行微调，即调整模型的部分参数，以适配我们的训练任务。最后，通常添加一层（或多层）输出层来替换源模型的最后一层，以满足新任务的输出要求。3.介绍建立迁移学习模型的简要流程：首先import 源模型，并建立其base_model，继承源模型权重并去掉top层，为其接上全连接层以及输出层，提取出特征层建立为新model，并且要将迁移的网络设置为不可训练来冻结源模型的参数。4.介绍了迁移模型的三个推广：1.增量学习2.多任务学习3.辅助任务学习。

## 第七次课 10月28日

今天课程主要涉及两部分的内容：经典卷积神经网络-AlexNet和ResNet。下面是今天的课程纪要： 

第一部分：经典卷积神经网络-AlexNet. 
1. AlexNet简介：AlexNet是2012年ImageNet竞赛冠军获得者Hinton和他的学生Alex Krizhevsky设计的，其top5预测的错误率为18.9%，是ImageNet竞赛中第一个使用神经网络的参赛者。在那年之后，更多的更深的神经网路被提出。ImageNet是由斯坦福大学李菲菲教授团队构建的大型图片数据集，ImageNet2012是其中使用最多的子数据集，它包含1281167张训练图片、50000张验证图片、100000张测试图片，分类任务为1000分类。 
2. AlexNet的主要创新点：
(1) 使用ReLU作为激活函数。在过去的神经网络中，人们更喜欢使用Sigmoid这类连续可导的激活函数，而ReLU在0点不可导。这种不光滑的激活函数与统计学中的LASSO惩罚项具有异曲同工之妙，LASSO采用绝对值惩罚项函数同样也是不光滑的函数，它能使得统计模型的产生稀疏的参数估计。ReLU激活函数同样能够使得神经元的输出变得稀疏，在实验中ReLU的效果被证明超过了Sigmoid；
(2) 训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合，一般在全连接层使用; 使用重叠的最大池化。
(3) 提出LRN(Local Response Normalization，即局部响应归一化)层，逐渐被BN（Batch Normalization）所代替; 
(4) 使用CUDA加速神经网络的训练，利用了GPU强大的计算能力。受限于当时计算能力，Alexnet使用两块GPU进行训练; 
(5) 简单的数据增强，随机的原图中截取更小的区域(以及水平翻转的镜像)。 
3. AlexNet的模型结构：包含输入层，8个卷积池化层，2个全连接-Dropout层以及输出层，请同学们逐层分析模型结构。原始的AlexNet适用于1000分类问题，共包含约6000万参数。我们为大家提供的案例为2分类问题，因此需要对输出层进行一点改造。 

第二部分：经典卷积神经网络-ResNet. 
1. ResNet简介：ResNet是2015年ImageNet竞赛冠军获得者何恺明、张祥雨、任少卿、孙剑四位华人学者共同设计的卷积神经网络，ResNet152的top5预测的错误率仅为5.62%。经典的神经网络结构中，上一层的输出被当做下一层的输入，这就使得整个神经网络的模型可以被视为一个极其复杂的复合函数。在反向传播的求导过程中，由于链式法则，越靠前的层梯度经历的连乘积越多，因而很容易出现梯度消失或者爆炸的问题。为了解决这个由深度引起的问题，He et al. 提出了残差学习模块，并靠大量的残差学习模块串联得到了超过100层的深度卷积神经网络。 
2. ResNet的主要创新点-残差学习模块: 对于一个两层的残差学习模块，我们假设两层卷积分别为f1和f2，则输出为f2(f1(x))，此时通过链式法则计算导数为 f'2(f1(x)) * f'1(x)。而通过快捷连接后的输出变为f2(f1(x)+x)，此时通过链式法则计算导数为 f'2(f1(x)) * [f'1(x)+1]。可以看到resnet通过给梯度强制加1来缓解梯度消失的问题。
3. 关键技术操作：(1) 卷积池化；(2) 尺寸变换，ResNet除了开头和最后通过池化改变尺寸外，残差块之间的降采样均通过步长为2的卷积实现；(3) 张量加和，需要保证加和的张量尺寸完全一致，这也是ResNet中间使用的卷积全部为same的原因。
4. ResNet模型结构：ResNet后面的数字序号代表模型中包含的卷积池化层。例如ResNet18包含8个残差块以及开头的卷积池化，共8*2+2=18层。原始的ResNet18适用于1000分类问题，共包含约3000万参数。我们为大家提供的案例为10分类问题，因此需要对输入和输出的部分层进行一点改造。 

最后，再次感谢大家的辛苦努力，可以看到大家都有巨大的进步，加油！

## 第八次课 11月4日

亲爱的各位同学，我们之前已经学习了深度学习的基本模型和编程方法，从今天开始我们学习怎么把这些方法用起来，会涉及到一些工程化的编程技巧。

第一部分：局部二次逼近算法

1. 问题背景：深度学习模型通常拥有超高维度的参数，这使得他的优化成为了一个十分痛苦的体验。对于随机梯度下降法而言，学习率的选择似乎是一个很重要的因素。该如何进行学习率的选择，使得它能够帮助我们加速算法的收敛，同时我们希望它是一个自适应的算法，并不需要进行额外的设置。
2. 算法提出：我们学习了Nestrov、Momentum等经典的数值优化算法。然后把高维损失函数的优化问题变为一个关于学习率的问题，在每一次参数更新步骤中，通过局部二次逼近方法，用学习率的标准二次函数形式来近似损失函数在梯度方向上的变化，以一种高效的方式获得接近最优的学习率。
3. 通过这个案例，我们展示了在实际业务问题中，如何找到痛点，发现并解决一个实际问题的过程。

第二部分：熊头识别案例

1. 案例背景：从今天的课程开始，我们逐步深入，为大家讲述自动驾驶的视频方案背后目标检测的学术思想。对于一般的目标定位问题，我们有一张很大的背景图片，而我们感兴趣的目标可能很小，只出现在背景图片的某个部位，我们该如何识别呢？一种思路是把背景图片当X，把目标所在位置信息当Y(例如物体框的坐标)，比较麻烦的问题是我们需要给图片进行标注。第二个问题是，我们要怎样建立这样的X和Y之间的关系？第三个问题是，以现有的资源我们能否处理高分辨率的图片？
2. 识别思路：我们想要自动的识别图片中的熊头，我们尝试了这种大图切小图的思路。我们给大家提供一个简单直观的解决思路：对大图进行切割，切割成小图片进行处理。这种方法同样需要进行图片的标注，但是我们可以把X和Y的关系转化为分类问题(物体在不在小图中)，而且小图也是我们的计算资源可能承担的。
3. 上机实验：实验代码主要分为三部分，首先我们在猫狗图像上打上熊头水印；然后裁切图像成为小块的XX和对应标签YY、训练一个二分类深度学习模型；最后对于一张大图，我们切分成小图进行预测，找到最可能包含熊头的位置。


## 第十次课：交通标志牌收尾+CUB200鸟类目标检测

亲爱的各位同学，大家好，以下为本次课程的课程纪要：

【1】今天的课程首先对交通标志牌案例进行收尾。对于任意的一张可能含有交通标志牌的图片960x1280，我们如何定位其中的交通标志牌呢？
- 首先我们采取VGG16模型来提取原图的特征图，接着使用第二部分训练好的模型来预测Y_hat，此时Y_hat是30*40的大小，这与原图尺寸不同，因此接下来采用上采样（upsampling）的方法将Y_hat的大小变为960x1280，根据Y_hat的结果去定位原图中可能存在的交通标志牌。问题在于预测出的标注框可能不贴合真实的标注框。
- 本次课提供了一个类似yolo的标注框回归方法，其中根据标注框的坐标计算得到标注框的中心位置，因此就可以利用30*40的子图来预测其中的标注框的中心位置以及长宽，特别的我们需要知道的是相对于子图的中心位置，而非相对于大图的中心位置。

【2】课堂的第二部分开启了另一个目标检测的案例——CUB200鸟类分类数据集。
一、数据介绍：(1) 简介：CUB200是一个非常著名的细粒度分类的标准数据集。它将鸟类细分为200类，训练集5994张，测试集5794张。（2）文件介绍：【Images.txt】：存储图片的类别和存储路径；【bounding_boxes.txt】：存储图片的标注框信息；所有图片的标号是共享的；【part_locs.txt】：存储每张图片上鸟的15个部位的位置信息。
二、数据准备：(1) 目的：识别鸟类特定part的位置。(2) 操作方案：对每一张子图，随机生成大量小子图。如果子图包含我们关心的part的中心点，则将改子图记为正例，否则记为负例。由此我们将目标检测规范化为图像分类问题。(3) 技术细节：由于数据集中的part较小，因此我们在随机截图的过程中会产生大量的负例。为此，我们要对负例进行一定的降采样。这是我们对计算资源有限的妥协。
三、子图模型训练：迁移VGG16模型进行模型的训练。
四、预测应用：(1) 对任意一张图片，生成大量随机切分的子图，对每一张子图进行预测；(2) 为了综合所有子图的预测信息，我们将子图的概率不断的累加到原图所对应的位置。这样，有目标的区域会有更多的子图进行“投票”，从而获得更大权重，最终我们再将概率图进行标准化得到一张0-1取值的概率热力图。(3) 通过对概率热力图设置截断点，我们可以取出我们预测的区域。

以上是本次课堂的主要内容。简要总结一下，我们在本周主要关注了计算机视觉领域的前沿问题：目标检测。在过去的十几年内，许许多多的目标检测模型被提出，例如RCNN，YoLo等。我们抛开这些方法的复杂实现，为大家简化提炼了其中的一种具有直觉的做法：利用大量备选的局部子图，来进行精细的识别，从而将识别问题转化为更加简单的图像分类问题和回归问题。虽然这些方法比较“简陋”，但是它能够提供非常直观的结果，也能在大家日后自学目标检测方法的时候提供一些基础帮助。