# 课堂纪要

## 第一次课 9月16日

各位亲爱的同学们大家好，首先非常欢迎大家选修汉生老师的《深度学习与人工智能》课程，这个课程的火爆程度超出了我们的想象。以下是今天的课程内容的简介，主要包括两个部分：
1. 第一部分：深度学习简介
	深度学习作为一种非线性回归，与经典的线性回归的区别在于我们选用了特殊的X（例如图像），并且建立了从X到Y的回归关系，例如人脸识别中的X是人脸，Y是人的身份，从X到Y建立回归关系后就可以判断这张人脸照片是否是某位特定的人
2. 第二部分：图像处理
	图像处理，包括将图片读入计算机、图片展示、对图片进行代数运算等

## 第二次课 9月23日

各位亲爱的同学们大家好！感谢大家今天的努力学习。接下来对今天的知识要点做一个总结。我们主要讲了如何通过Tensorflow实现线性回归。
1. 第一部分：线性回归回顾
   （1）线性回归模型形如：Y = Xb + e，用来处理连续响应变量的问题。（2）建立深度学习模型的方法论流程为：(a) 定义X和Y；(b) 定义X和Y之间的函数形式f，在线性回归中是线性的、将来会很复杂；(c) 定义损失函数来评价模型好坏；(d) 优化求解参数，在深度学习中不同的优化算法结果也会不同。 
2. 第二部分：数据准备工作
   （1）准备X数据： 首先初始化一个tensor X，采用Image.open()，从文件夹路径中循环读入图片，将其转换成numpy array，然后放入X中。由于我们的数据比较小，所以我们目前可以采用这样的方式读取数据，当数据量过大时，我们需要一块一块的读取数据。（2）准备Y数据：将csv文件中的因变量转换成矩阵(N*1)后赋值给Y，并对其进行中心化、标准化。（3）切分训练集和验证集：从sklearn.model_selection导入train_test_split函数，采用train_test_split()，将数据一分为二，一部分训练模型，一部分验证模型。
3. 第三部分：Tensorflow实现
   （1）建立模型：从Keras库中导入所需要的层和Model，进行相应操作。（2）模型结构：模型包括Input层、Flatten层和Dense层，Input层输入tensor的维度和对应size（第一个维度和样本量或batch_size对应），Flatten层将维数复杂的tensor拉直成一维向量，Dense层将输入的全体神经元以线性组合的方式全连接到一个节点输出。最后将input_layer和output_layer交给Model声明，我们就定义了一个f(X)。Tensorflow模型的流程特别像组装水管系统，数据是在水管（模型）中流动的水，不同的层是水管和水管之间的阀门，它们对流经此处的数据流进行相应的处理，最终在水管的末端得到输出，也就是f(X)。（3）施工方案：通过命令model.compile()，我们可以指定优化所需要的损失函数（MSE），优化器（Adam或SGD）和监测方案（检测指标MSE）。

## 第三次课 9月30日

大家好！感谢大家今天的努力学习。接下来对今天的知识要点做一个总结。我们主要讲了如何通过Tensorflow实现逻辑回归。
1. 一、逻辑回归回顾：
   不同于线性回归，逻辑回归中我们的Y是一个分类型的因变量。（1） 逻辑回归模型形如：
    $$ P(Y=k)=\exp(X^\top \beta_k)/ \sum_{k'} \exp(X^\top \beta_{k'}), $$ 
   在深度学习任务中X通常不是向量而是一个Tensor，我们需要参数的形状需要和X进行对齐(也需要是一个Tensor)。（2）建立深度学习模型的方法论流程为：(a) 定义X和Y，(b) 定义X和Y之间的函数形式f，(c) 定义损失函数来评价模型好坏，(d) 优化求解参数。 

2. 二、数据准备工作：
   （1）准备X数据： 首先初始化一个tensor X，采用Image.open()，从文件夹路径中循环读入图片，将其转换成矩阵，然后放入X中。由于我们的数据比较小，所以我们目前可以采用这样的方式读取数据，当数据量过大时，我们需要一块一块的读取数据。同学们要尝试理解硬盘存储、内存和显存的关系。（2）准备Y数据：将csv中的因变量转换成矩阵(N*1)后赋值给Y（应用to_categorical()将其转化为One-Hot型因变量）。（4）切分训练集和验证集：从sklearn.model_selection 导入 train_test_split，采用train_test_split()，将数据一分为二，一部分训练模型，一部分验证模型。
3. 三、Tensorflow的实现：
   （1）建立模型：从Keras库中导入所需要的层和Model，进行相应操作。（2）模型结构：模型包括Input层、Flatten层和Dense层，Input层输入tensor的维度和对应size（第一个维度和样本量or batch_size对应），Flatten层将维数复杂的tensor拉直成一维向量，Dense层将输入的全体神经元以线性组合的方式全连接到一个节点输出。最后将input_layer 和 output_layer叫给Model声明，我们就定义了一个f(X)。Tensorflow模型的流程特别像组装水管系统，数据是在水管（模型）中流动的水，不同的层是水管和水管之间的阀门，它们对流经此处的数据流进行相应的处理，最终在水管的末端得到输出，也就是f(X)。（4）施工方案：通过命令model.compile()，我们可以指定优化所需要的损失函数（因变量为One-Hot型使用"categorical_crossentropy"，否则使用"sparse_categorical_crossentropy"），优化器（Adam或SGD）和监测方案（检测指标"accuracy"）。

## 第四次课 10月7日

恭喜大家又勇敢地向前走出了一大步，相信后面的学习会越来越顺畅。今天主要讲了三个部分：梯度下降、卷积以及池化

1. 第一部分：梯度下降
我们从之前学过的简单模型：线性回归和逻辑回归出发，统一到优化损失函数的理论框架。我们从理论上建立了学习率大小和数值收敛的约束关系。

2. 第二部分：卷积 
   - 什么是卷积？为什么需要进行卷积操作？卷积就是计算某种局部特征的相似性。简单来说，有一个记录了目标图像局部特征的像素矩阵，称作【卷积核（convolution kernel）】或者【滤波器（filter）】。 我们通过其与搜索区域进行对应位置的点乘再求和运算，来量化两者之间的相似度。如果将卷积核先行后列地扫描某一图像的像素矩阵并进行运算，由此得到一个新的像素矩阵，整个过程就叫做卷积。 注意：因为卷积核一般关注的是局部特征，所以尺寸一般比较小，如3*3，5*5等。图像具有某种平移不变形，例如不同位置的熊头都是熊头，使用卷积可以衡量这种平移不变形。此外，如果所有模块全都使用全连接层，消耗的参数过多，而卷积相对而言较为“节约”。
   - 常见的卷积操作有full, same和valid卷积。这三种卷积其实是对卷积核移动范围的不同限制。 （a）full卷积：只要像素矩阵与卷积核元素有一个位置重叠，就要将对应位置的值相乘再求和（落在像素矩阵外的元素视为0） （b）same卷积： 能使输入和输出的像素矩阵尺寸一样（但不一定总是一样，和卷积核的步长也有关） （c）valid 卷积：要求卷积核全部被像素矩阵覆盖时，才能进行卷积运算 
   - 多通道卷积：如果输入的像素矩阵是多通道的，卷积核的通道个数要与输入保持一致。具体做法是分别在每一层通道上进行二维卷积，然后再进行深度方向的求和，最终输出一个单通道的矩阵，因此，该操作具有降维的作用。
   - 一个张量与多个卷积核的卷积：通常会使用多个卷积核进行卷积提取图像特征，再把每个卷积核的输出进行叠加，形成新的输出。最后输出的通道的个数与卷积核的个数一致。 一般操作之后，一个像素高通道数小的张量，变成像素低通道数大的张量，这样就提取到了更多的深层特征。 
   - 卷积的权重如何确定：卷积的权重是未知的参数，对于该参数的估计，需要先构造一个损失函数，根据某算法（比如SGD）优化后（即最小化损失函数），就得到卷积权重的估计。 

3. 第三部分：池化： 
   - 卷积和池化往往会同时出现，池化操作是对卷积得到的结果进一步处理，它是将平面内某一位置及其相邻位置的特征值进行统计汇总，并将汇总后的结果作为这一位置在该平面内的值输出。如果取最大值，则称为最大值池化，如果取平均值，则称为平均值池化。最常用的为最大池化，即在卷积中计算出图像局部特征的相似度，再通过池化挑出最突出的特征。 
   - 池化可以将维数大大降低，输出的维数=输入的维数/池子的大小。另外，池化不消耗参数。 
   - 与卷积一样，池化也分为same池化和valid池化等。
   - 多通道的池化：与卷积不同，通常池化会在每一通道上分别进行池化，最后输出的通道数和输入的通道数一致。

感谢大家的辛苦付出，继续加油！

## 第五次课 10月14日

亲爱的各位同学，大家好！我们今天的课程进入到了卷积神经网络（CNN）的世界，介绍了一个经典的CNN模型：LeNet5，以及一种重要的训练技巧：数据增强。

1. 第一部分：LeNet5模型
   - LeNet5是1998年由Yann LeCun等人提出的用于手写数字识别的CNN网络，是最经典的CNN网络之一。LeNet5的网络结构详解，包括各层的输出大小和消耗的参数个数时，同学们需要认真理解。易混淆点：卷积层输出的通道个数与卷积核的个数一致，池化层输出的通道个数与输入的通道个数一致。
   - Dense层为全连接层，这种结构产生了大量的参数，因此考虑使用Dropout将模型简化，即随机丢弃用于提取特征的神经元，进而减少模型需要估计的参数。
   - 目前使用的LeNet5 结构与最初提出的结构有所不同，例如选用的激活函数。现在一般使用 ReLU 作为激活函数，输出层则为 softmax。ReLU（Rectified Linear Units）为分段线性函数，函数形式为R(x)=max(x,0)。ReLU的特点在于，它能使某些神经元的输出为0，继而实现网络的稀疏性和输出的稳定。从统计学的角度来看，ReLU是一类样条函数，其线性组合可以逼近任意线性或非线性函数，从而增加神经网络的非线性表达能力。
2. 第二部分：数据增强（Data Augmentation）
   - 数据增强的目的在于，我们拿到的图像数据往往只是一个角度。为了让模型能够识别不同角度的相同图像，我们需要喂给模型不同角度的图像作为输入，这通过图像变换来实现。
   - 这些图像变换包括：放大/缩小、平移、旋转、拉伸等等。
   - 在代码实现上，我们采用Keres的数据生成器ImageData Generator批量地生成数据增强后的图像。数据生成器是一个类似搬运工的角色，他可以完成：按批量读取数据、数据归一化、统一图像尺寸、数据增强等一系列准备工作，便于我们后续训练。
   - 我们在处理训练数据时，需要进行数据增强；但在处理验证数据集时，不需要数据增强。

最后提醒一下，本周作业TASK5的截止时间是【10月21日】。
感谢大家的辛苦付出，继续加油！
